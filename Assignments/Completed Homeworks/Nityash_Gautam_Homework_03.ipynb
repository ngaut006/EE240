{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfyvKULWGIUs"
   },
   "source": [
    "# **ASSIGNMENT 3** EE:240 PATTERN RECOGNITION AND MACHINE LEARNING\n",
    "  *NAME: NITYASH GAUTAM*\n",
    "  \n",
    "  *SID: 862395403*\n",
    "\n",
    "  *UCR Email: ngaut006@ucr.edu*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lCOZJWTatXo"
   },
   "source": [
    "## **ESSENTIAL IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHrtq8RhaydG"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGsVRkZMS57Y"
   },
   "source": [
    "## **H 3.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjkTLkGBTA3d"
   },
   "source": [
    "### **H 3.1 (a) Linear System** - (0 pts)\n",
    "\n",
    "Let us consider a Linear Model $y = Wx + b$, where *W* is an *m x d* matrix and $b ∈ \\mathbb{R}$. Write and expression for the optimal values of *W, b* that minimize the following cose function:\n",
    "\n",
    "$$J(W,b) = \\frac{1}{2}||y-Wx-b||^2$$\n",
    "\n",
    "Remember the optimality condition that the gradient of the cost function w.r.t. the optimization variables should be zero at their optimal values. Therefore first derive expressions for $\\frac{δJ}{δW_{ij}}$, $\\frac{δJ}{δb}$, where $W_{ij}$ denotes *i,j* entry in matrix *W*. Then set the gradients to zero to derive the expressions for the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWQJZs4KU-ME"
   },
   "source": [
    "**SOLUTION**\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Starting off by the calculating the partial derivatives for the cost function J(W,b).\n",
    "\n",
    "As the cost function is a squared Euclidean Norm, it can be re-written as follows:\n",
    "\n",
    "$$J(W,b) = \\frac{1}{2}(y - Wx - b)^T(y - Wx - b)$$\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "*Computing Partial Derivatives*\n",
    "\n",
    " * Derivating J w.r.t. $W_{ij}$:\n",
    " \n",
    "  $\\frac{δJ}{δW_{ij}} = -(y - Wx - b)^T \\frac{δ}{δW_{ij}}(Wx + b)$\n",
    "\n",
    "  Since $Wx + b$ is a linear function w.r.t. $W_{ij}$, we have $\\frac{δ}{δW_{ij}} (Wx + b) = x_j$ (i.e., the j-th element of the vector x). Therefore, we get\n",
    "\n",
    "  $\\frac{δJ}{δW_{ij}} = -(y - Wx - b)^Tx_j$\n",
    "\n",
    "  Summing over all j (since i is fixed) to get the derivative with respect to the i-th row of W, we have\n",
    "\n",
    "  $\\frac{δJ}{δW_i} = -(y - Wx - b)^Tx$\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    " * Derivating J w.r.t $b$ :\n",
    "\n",
    "  $\\frac{δJ}{δb} = -(y - Wx - b)^T$\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Equating both the derivatives to zero gives us the optimal values of W and b\n",
    "\n",
    "$W^*x = y - b^*$\n",
    "\n",
    "$b^* = y - W^*x$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGllkQ3KqYfo"
   },
   "source": [
    "### **H 3.1 (b) CONVOLUTIONAL SYSTEM** - (10 pts)\n",
    "Let us consider the following linear model $y = Wx+b$, where $W$ is a Toeplitz matrix corresponding to a filter $w$ of length $k$. We can write the *d+k−1 X d* Toeplitz matrix as\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    w_1 & 0 & 0 & ⋯ & 0 & 0 \\\\\n",
    "    w_2 & w_1 & 0 & ⋯ & 0 & 0 \\\\\n",
    "    w_3 & w_2 & w_1 & ⋯ & 0 & 0 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "    w_k & w_{k-1} & w_{k-2} & ⋯ & ⋯ & ⋯ \\\\\n",
    "    0 & w_k & w_{k-1} & ⋯ & ⋯ & ⋯ \\\\\n",
    "    0 & 0 & w_k & \\ddots & ⋯ & \\vdots \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "    0 & 0 & 0 & ⋯ & w_k & w_{k-1} \\\\\n",
    "    0 & 0 & 0 & ⋯ & 0 & w_k \\\\\n",
    "  \\end{bmatrix}$$\n",
    "\n",
    "We can also define the linear model above as $y = w*x+b$, where $x*w$ denotes the convolution of two vectors that can be defined as\n",
    "\n",
    "$$(x*w)_j = ∑_{i=1}^dx_iw_{j-i+1}$$\n",
    "\n",
    "where wi denotes $i^{th}$ entry in vector $w$ and $w_i = 0$ for i < 1 and i > k. With our definition above, we assume that j is only valid for $1 ≤ j ≤ d + k − 1$.\n",
    "\n",
    "Derive an expression for the optimal values of w, b that minimize the following cost function:\n",
    "\n",
    "$$J(W,b) = \\frac{1}{2}||y - w*x - b||^2$$ - (1)\n",
    "\n",
    "Also, derive the expressions for $\\frac{δJ}{δw_i}$ and $\\frac{δJ}{δb}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34abi_H1YJKm"
   },
   "source": [
    "### H 3.1 **(c) DECONVOLUTION** - (10 pts)\n",
    "\n",
    "In this question, you will estimate a filter $w$ by minimizing the cost function in $[J(W,b) = \\frac{1}{2}||y - w*x - b||^2]$ via gradient descent using an input-output pair $(x, y)$.\n",
    "\n",
    " * y = [1,1,1,1,1,1,1,1,1,-9] and x = [1,2,....,9] is a vector of length 9. The filter $w$ is of length 2. Compute $w ∈ \\mathbb{R}^2$ and $b ∈ \\mathbb{R}$ using gradient descent.\n",
    "\n",
    " * y = [1,5,2,3,4,5,6,7,8,9,5.5] and x = [1,2,....,9] is a vector of length 9. The filter $w$ is of length 3. Compute $w ∈ \\mathbb{R}^3$ and $b ∈ \\mathbb{R}$ using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fG0662p_a7SI"
   },
   "source": [
    "#### *Helper Functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oii01B2GsZca"
   },
   "outputs": [],
   "source": [
    "def compute_grads(x, y, w, b):\n",
    "  predicted_y = np.convolve(w, x, 'full') + b\n",
    "  error = y - predicted_y\n",
    "\n",
    "  # Calculating gradients\n",
    "  w_grad = -1 * np.convolve(x, error, 'valid')\n",
    "  b_grad = -1 * np.sum(error)\n",
    "\n",
    "  return w_grad, b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfQnOIy_bg4S"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, len_w, lr=0.001, epochs=1000):\n",
    "    w = np.zeros(len_w)\n",
    "    b = 0.0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        grad_w, grad_b = compute_grads(x, y, w, b)\n",
    "\n",
    "        # update parameters\n",
    "        w = w - lr * grad_w\n",
    "        b = b - lr * grad_b\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sa9iDJMPbs6o"
   },
   "source": [
    "#### *Executing Example 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ojv58DLVbqUk",
    "outputId": "cf39a117-500f-47ba-ea10-319970a2d93d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized values of filter w: [ 0.96134719 -0.96935766]\n",
      "Optimized values of bias b: 0.05904710059462741\n"
     ]
    }
   ],
   "source": [
    "y = np.array([1,1,1,1,1,1,1,1,1,-9])\n",
    "x = np.arange(1, 10)\n",
    "len_w = 2\n",
    "\n",
    "w_opt, b_opt = gradient_descent(x, y, len_w)\n",
    "print(\"Optimized values of filter w:\", w_opt)\n",
    "print(\"Optimized values of bias b:\", b_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwaaV201LexH"
   },
   "source": [
    "#### *Executing Example 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xSDp9vsvb3ab",
    "outputId": "50db5b8c-c510-438e-9448-f86d63fb994f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized values of filter w: [-3.04983029  4.6360835  -1.81663934]\n",
      "Optimized values of bias b: 5.8227176307377375\n"
     ]
    }
   ],
   "source": [
    "y = np.array([1,5,2,3,4,5,6,7,8,9,5.5])\n",
    "x = np.arange(1, 10)\n",
    "len_w = 3\n",
    "\n",
    "w_opt, b_opt = gradient_descent(x, y, len_w)\n",
    "print(\"Optimized values of filter w:\", w_opt)\n",
    "print(\"Optimized values of bias b:\", b_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXfcazBaMBPY"
   },
   "source": [
    "## **H 3.2** In this question, we will learn how to implement backpropagation (or backprop) for “vanilla” neural networks (or Multi-Layer Perceptrons).  (60 pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfRVuFdqo4mw"
   },
   "source": [
    "---\n",
    "\n",
    "[CLICK HERE](https://drive.google.com/drive/folders/1R1vUviNUh_V25-m5GXTyJ3n6EkahHfQG?usp=share_link) TO ACCESS THE ZIP FILE\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA7hNpsdD8uR"
   },
   "source": [
    "## **H 3.3** In this question you will solve the XOR problem using 2-layer neural network using ReLu nonlinearities. Remember that in the class, we used step/sign nonlinearities. (20 pts)\n",
    "\n",
    "Suppose you are given four training samples from $\\mathbb{R}^2$ distributed in an XOR pattern ((i.e., points on opposite diagonals of a rectangle have same labels, as shown in the table)\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    y & x(1) & x(2) \\\\\n",
    "    1 & -1 & -1 \\\\\n",
    "    0 & -1 & 1 \\\\\n",
    "    0 & 1 & -1 \\\\\n",
    "    1 & 1 & 1 \\\\\n",
    "  \\end{bmatrix}$$\n",
    "\n",
    "We want to use a 2-layer neural network shown in Figure 2 to classify this data using ReLu nonlinearity $ϕ(z) = max(0, z)$.\n",
    "\n",
    "We use same notations we discussed in class, where superscripts 1,2 indicate layer number (not the power).\n",
    "\n",
    "$$ z^1 = \\begin{bmatrix}\n",
    "    z_1^1 \\\\\n",
    "    z_2^1 \\\\\n",
    "  \\end{bmatrix} = \\begin{bmatrix}\n",
    "    w_{11}^1 & w_{12}^1 \\\\\n",
    "    w_{21}^1 & w_{22}^1 \\\\\n",
    "  \\end{bmatrix} \\begin{bmatrix}\n",
    "    x(1) \\\\\n",
    "    x(2) \\\\\n",
    "  \\end{bmatrix} + \\begin{bmatrix}\n",
    "    b_1^1 \\\\\n",
    "    b_2^1 \\\\\n",
    "  \\end{bmatrix} = W^1x + b^1,$$\n",
    "\n",
    "$$z^2 = \\begin{bmatrix}\n",
    "    w_{11}^2 & w_{12}^2 \\\\\n",
    "  \\end{bmatrix} \\begin{bmatrix}\n",
    "    y^1_1 \\\\\n",
    "    y^1_2 \\\\\n",
    "  \\end{bmatrix} + b^2 = W^2y^1 + b^2,$$\n",
    "\n",
    "$$y^2 = ϕ(z^2)$$\n",
    "\n",
    "Our goal is to find some values for weights W1,W2, b1, b2 such that the data can be classified correctly. You can pick any values that can correctly classify data using ReLu nonlinearities: $ϕ(z) = max(0, z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xe2ZrGTqKYU3"
   },
   "source": [
    "$$W^1 = \\begin{bmatrix}\n",
    "    1 & 1 \\\\\n",
    "    1 & -1 \\\\\n",
    "  \\end{bmatrix}$$\n",
    "\n",
    "$$W^2 = \\begin{bmatrix}\n",
    "    1 & 1 \\\\\n",
    "  \\end{bmatrix}$$\n",
    "\n",
    "$$B^1 = \\begin{bmatrix}\n",
    "    -1\\\\\n",
    "    -1 \\\\\n",
    "  \\end{bmatrix}$$\n",
    "\n",
    "$$B^2 = 0$$\n",
    "\n",
    "[CLICK HERE](https://drive.google.com/file/d/1jF1GiG4r7Jqrla0YTQDNIHC0qnpggdOm/view?usp=share_link) TO SEE THE WORKING AND PLOTS"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
